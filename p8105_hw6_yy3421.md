p8105_hw6_yy3421
================

## Problem 1

``` r
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown"))  |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c( "Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (8): uid, victim_last, victim_first, victim_race, victim_sex, city, stat...
    ## dbl (4): reported_date, victim_age, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

In the data cleaning code below we create a `city_state` variable. Using
the mutate function, `victim_age` is transformed into numerical values
and `resolution` is created based on the `disposition` variable. Then,
`victim_race` and `city_state` variables are each filtered based on this
problem requirement. At the end, five variables above are selected for
further analysis.

``` r
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

|    OR | OR_CI_lower | OR_CI_upper |
|------:|------------:|------------:|
| 0.426 |       0.325 |       0.558 |

In the above code chunk, a logistic regression model is used for data
from Baltimore, MD using `resolved` as the outcome and `victim_age`,
`victim_sex`, and `victim_race` as predictors. After saving the output,
estimate and confidence interval of the adjusted odds ratio is obtained
after using `broom::tidy` and `mutate` for comparison between non-white
victims and white victims.

``` r
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

| city_state      |    OR | OR_CI_lower | OR_CI_upper |
|:----------------|------:|------------:|------------:|
| Albuquerque, NM | 1.767 |       0.831 |       3.761 |
| Atlanta, GA     | 1.000 |       0.684 |       1.463 |
| Baltimore, MD   | 0.426 |       0.325 |       0.558 |
| Baton Rouge, LA | 0.381 |       0.209 |       0.695 |
| Birmingham, AL  | 0.870 |       0.574 |       1.318 |

Next, through using`nest()`, `map()`, and `unnest()`, the model for
Baltimore is fitted for each of the cities. The adjusted odds ratio (and
CI) for solving homicides comparing non-white victims to white victims
for each cities are then obtained and stored.

``` r
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

<img src="p8105_hw6_yy3421_files/figure-gfm/problem 1 plot-1.png" width="90%" />
Above, a plot is generated for the estimated ORs and CIs for each city,
ordered by magnitude of the OR from smallest to largest. From this plot,
as most cities have odds ratios that are smaller than 1, it could be
intepreted that crimes with male victims have smaller odds of resolution
compared to crimes with female victims after adjusting for victim age
and race. However, there is a maximum disparity in New York. Further, a
narrow confidence interval in approximately half of the cities could
suggest a significant difference in resolution rates by sex after
adjustment for victim age and race.

## Problem 2

``` r
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

    ## using cached file: /Users/yyl/Library/Caches/org.R-project.R/R/rnoaa/noaa_ghcnd/USW00094728.dly

    ## date created (size, mb): 2023-09-28 10:20:08.386959 (8.524)

    ## file min/max dates: 1869-01-01 / 2023-09-30

After importing the weather data, a `boot_sample` function is first
created for generate bootstrap samples.

``` r
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
  
}
```

Below, using the `boot_sample` and `map` functions, 5000 boot samples
are created and stored in the `boot_straps` dataframe. Using the `map`
and `lm` function, linear models using `tmax` as the outcome and
`tmin`and `prcp` as predictors are stored in the `models` variable.

Next, using `map` and `broom::glance`, we can generate results of the
linear model for each boot sample. After using the `unnest` function to
show this result, we can specifically select the `r.squared` variable to
obtain the estimates. Similarly, using `map` and `broom::tidy`, we can
obtain estimates for β0, β1, and β2.

``` r
boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df)),
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df))
  )

boot_results_1 = 
  boot_straps |> 
  mutate(
    results_1 = map(models, broom::glance)
  ) |> 
  select(strap_number, results_1) |> 
  unnest(results_1) |> 
  select(strap_number, r.squared)

boot_results_2 = 
  boot_straps |> 
  mutate(
    results_2 = map(models, broom::tidy),
    results_2 = map(results_2, as_tibble)
  ) |> 
  select(strap_number, results_2) |> 
  unnest(results_2) |>
  select(-std.error,-statistic, -p.value) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  )
```

The two results of r.squared and β values are combined into one data
frame using the left_join function. Using the mutate function, log(β0β1)
andlog(β1β2) are calcualted for each boot sample.

``` r
boot_results = 
  left_join(boot_results_1, boot_results_2, by = "strap_number") |> 
  janitor::clean_names() |> 
  rename(beta_zero = intercept,
         beta_one = tmin,
         beta_two = prcp) |> 
  mutate(
    logbeta01 = log(beta_zero * beta_one),
    logbeta12 = log(abs(beta_one * beta_two))
  )
```

### Distribution of R.Squared and Log(β1\*β2)

The distribution for R.Squared and Log(β1\*β2) are plotted below. We can
observe a tail for smaller estimates for both two quantities, with a
heavier tail for estimates of R.squared, which could be an indication
for outliers in the bootstrap samples.

``` r
boot_results |> 
  ggplot(aes(x = r_squared))+
  geom_density()+
  labs(
    x = "R Squared",
    caption = "Distribution of Estimates of R.Squared"
  )
```

<img src="p8105_hw6_yy3421_files/figure-gfm/plots problem 2-1.png" width="90%" />

``` r
boot_results |> 
  ggplot(aes(x = logbeta12))+
  geom_density()+
  labs(
    x = "Log(β1*β2)",
    caption = "Distribution of Estimates of Log(β1*β2)"
  )
```

<img src="p8105_hw6_yy3421_files/figure-gfm/plots problem 2-2.png" width="90%" />

### 2.5% and 97.5% Quantiles Summary for R Squared and and Log(β0\*β1)

Using `summarize()`, we can generate 2.5% and 97.5% quantiles for
R.squared and Log(β0\*β1).

``` r
boot_results |> 
  summarize(
    ci_lower_r = quantile(r_squared, 0.025),
    ci_upper_r = quantile(r_squared, 0.975),
    ci_lower_beta01 = quantile(logbeta01, 0.025),
    ci_upper_beta01 = quantile(logbeta01, 0.975)
  )
```

    ## # A tibble: 1 × 4
    ##   ci_lower_r ci_upper_r ci_lower_beta01 ci_upper_beta01
    ##        <dbl>      <dbl>           <dbl>           <dbl>
    ## 1      0.890      0.940            2.06            2.14

Based on the quantile summary, we can observe an overall high R squared
with its 2.5% quantile of 0.888 and 97.5% quantile of 0.940. This could
be an indication of smaller differences between the observed data and
the fitted values generated from the linear regression model.

Furthermore, it is 95% oissubke tgat the 2.5% quantile of 2.06 and 97.5%
quantile of 2.14 of Log(β0\*β1) could indicate an positive correlation
between tmin and tmax.

## Problem 3

### Data Cleaning

After importing data of birth weight, variables `babysex`, `malform`,
`frace`, and `mrace` are transformed into factor variables.

``` r
birthweight_df = 
  read_csv(
    "data/birthweight.csv", na = c("", "NA", "Unknown")
    ) |> 
  mutate(
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  ) 
```

    ## Rows: 4342 Columns: 20
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## dbl (20): babysex, bhead, blength, bwt, delwt, fincome, frace, gaweeks, malf...
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

    babysex = case_match(
      babysex, 
      1 ~ "male",
      2 ~ "female"
    ),
    babysex = as.factor(babysex),
    malform = case_match(
      malform, 
      0 ~ "absent",
      1 ~ "present"
    ),

### Proposed Linear Regression Model

``` r
linear_mod_all = lm(bwt ~ babysex + bhead + blength + bwt + delwt + fincome + frace + gaweeks+malform+menarche+mheight+momage+mrace+parity+pnumlbw+pnumsga+ppbmi+ppwt+smoken+wtgain, data = birthweight_df) |> broom::tidy()
```

Before proposing a linear regression model, a linear model using bwt as
the outcome and all other variables as predictors is generated below.
Through comparing p values of all predictors, we can observe that
variables `bhead`, `blength`, and `delwt` has smaller p values compared
to other variables. Based on these small p values, these three variables
are proposed for the first linear model.

``` r
birthweight_df_select = birthweight_df |> 
  select(bwt, blength, bhead, gaweeks, babysex, delwt)
  
linear_mod_1 = lm(bwt ~ bhead + blength + delwt+ bhead*blength*delwt, data = birthweight_df_select)
```

# After generating the first linear regression model, predictions and residues for this model are generated `using add_predictions` and `add_residues`. Next, a plot of model residuals against fitted values is generated below. We can observe a slow decrease in residues as fitted value increases.

``` r
birthweight_df_select |> 
  modelr::add_predictions(linear_mod_1) |> 
  modelr::add_residuals(linear_mod_1) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .5)+
  geom_smooth(color = "red", se=FALSE)
```

    ## `geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = "cs")'

<img src="p8105_hw6_yy3421_files/figure-gfm/my model residue pred plot-1.png" width="90%" />

### Model 2 and Model 3

``` r
linear_mod_2 = 
  lm(bwt ~ blength + gaweeks, data = birthweight_df_select) 
```

Using `lm()`, a second linear model is generated uisng `bwt` as the
outcome and `blength` and `gaweeks` as the predictors.

``` r
linear_mod_3 = 
  lm(bwt ~ bhead* blength + bhead* babysex+ bhead*blength*babysex, data = birthweight_df_select) 
```

For the third linear model is generated uisng `bwt` as the outcome and
`blength`, `bhead`, `gaweeks`, and their interactions as the predictors.

### Comparing the 3 Models:

After generating the three models, cross validation is used through
`croosv_mc` to compare the three models. First, `train` and `test`
variables are setted up.

``` r
birthweight_df_select |> 
  gather_predictions(linear_mod_1, linear_mod_2, linear_mod_3) |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```

<img src="p8105_hw6_yy3421_files/figure-gfm/compare 3 models-1.png" width="90%" />

``` r
cv_df =
  crossv_mc(birthweight_df_select, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Next, using `map()`, rmse values based on train and test samples are
obtained for each model and stored in the `cv_df` data frame.

``` r
cv_df = 
  cv_df |> 
  mutate(
    mod_1 = map(train, \(df) lm(bwt ~ bhead + blength + delwt+ bhead*blength*delwt, data = df)),
    mod_2 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    mod_3  = map(train, \(df) lm(bwt ~ bhead* blength + bhead* babysex+ bhead*blength*babysex, data = df))
    ) |> 
  mutate(
    rmse_1 = map2_dbl(mod_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2    = map2_dbl(mod_2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_3 = map2_dbl(mod_3, test, \(mod, df) rmse(model = mod, data = df)))
```

The average value of rmse of model 1 is 284.0834647. The average value
of rmse of model 2 is 329.7995885. The average value of rmse of model 3
is 287.5420566. Overall, model 1 has the lowest prediction error and
model 2 has the highest prediction error. This pattern could also be
observed from the plot below.

``` r
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

<img src="p8105_hw6_yy3421_files/figure-gfm/rmse plot-1.png" width="90%" />
