---
title: "p8105_hw6_yy3421"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

```{r data import 1 and cleaning}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown"))  |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c( "Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

In the data cleaning code below we create a `city_state` variable. Using the `mutate` function, `victim_age` is transformed into numerical values and `resolution` is created based on the `disposition` variable. Then, `victim_race` and `city_state` variables are each filtered based on this problem requirement. At the end, five variables above are selected for further analysis.

```{r Baltimore}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

In the above code chunk, a logistic regression model is used for data from Baltimore, MD using `resolved` as the outcome and `victim_age`, `victim_sex`, and `victim_race` as predictors. After saving the output, estimate and confidence interval of the adjusted odds ratio is obtained after using `broom::tidy` and `mutate` for comparison between non-white victims and white victims.

```{r all cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

Next, through using`nest()`, `map()`, and `unnest()`, the model for Baltimore is fitted for each of the cities. The adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims for each cities are then obtained and stored. 

```{r problem 1 plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Above, a plot is generated for the estimated ORs and CIs for each city, ordered by magnitude of the OR from smallest to largest. From this plot, as most cities have odds ratios that are smaller than 1, it could be intepreted that crimes with male victims have smaller odds of resolution compared to crimes with female victims after adjusting for victim age and race. However, there is a maximum disparity in New York. Furthermore, a narrow confidence interval in approximately half of the cities could suggest a significant difference in resolution rates by sex after adjustment for victim age and race.


## Problem 2

```{r data import 2}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

After importing the weather data, a `boot_sample` function is first created to generate bootstrap samples.

```{r boot_sample}

boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
  
}

```

Below, using the `boot_sample` and `map` functions, 5000 boot samples are created and stored in the `boot_straps` data frame. Using the `map` and `lm` function, linear models using `tmax` as the outcome and `tmin`and `prcp` as predictors are stored in the `models` variable. 

Next, using `map` and `broom::glance`, we can generate results of the linear model for each boot sample. After using the `unnest` function to show this result, we can specifically select the `r.squared` variable to obtain the estimates. Similarly, using `map` and `broom::tidy`, we can obtain estimates for β0, β1, and β2. 


```{r boot strap}
boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df)),
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df))
  )

boot_results_1 = 
  boot_straps |> 
  mutate(
    results_1 = map(models, broom::glance)
  ) |> 
  select(strap_number, results_1) |> 
  unnest(results_1) |> 
  select(strap_number, r.squared)

boot_results_2 = 
  boot_straps |> 
  mutate(
    results_2 = map(models, broom::tidy),
    results_2 = map(results_2, as_tibble)
  ) |> 
  select(strap_number, results_2) |> 
  unnest(results_2) |>
  select(-std.error,-statistic, -p.value) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  )

```

The two results of r.squared and β values are combined into one data frame using the `left_join` function. Using the `mutate` function, log(β0β1) andlog(β1β2) are calculated for each boot sample.

```{r result combination and calc}
boot_results = 
  left_join(boot_results_1, boot_results_2, by = "strap_number") |> 
  janitor::clean_names() |> 
  rename(beta_zero = intercept,
         beta_one = tmin,
         beta_two = prcp) |> 
  mutate(
    logbeta01 = log(beta_zero * beta_one),
    logbeta12 = log(abs(beta_one * beta_two))
  )
```

### Distribution of R.Squared and Log(β1*β2)

The distribution for R.Squared and Log(β1*β2) are plotted below. We can observe a tail for smaller estimates for both two quantities, with a heavier tail for estimates of R.squared, which could be an indication for outliers in the bootstrap samples.

```{r plots problem 2}
boot_results |> 
  ggplot(aes(x = r_squared))+
  geom_density()+
  labs(
    x = "R Squared",
    caption = "Distribution of Estimates of R.Squared"
  )

boot_results |> 
  ggplot(aes(x = logbeta12))+
  geom_density()+
  labs(
    x = "Log(β1*β2)",
    caption = "Distribution of Estimates of Log(β1*β2)"
  )
```

### 2.5% and 97.5% Quantiles Summary for R Squared and and Log(β0*β1)

Using `summarize()`, we can generate 2.5% and 97.5% quantiles for R.squared and Log(β0*β1). 

```{r quantiles summary}
boot_results |> 
  summarize(
    ci_lower_r = quantile(r_squared, 0.025),
    ci_upper_r = quantile(r_squared, 0.975),
    ci_lower_beta01 = quantile(logbeta01, 0.025),
    ci_upper_beta01 = quantile(logbeta01, 0.975)
  ) |> 
  knitr::kable(digits = 3)
```

Based on the quantile summary, we can observe an overall high R squared with its 2.5% quantile of 0.888 and 97.5% quantile of 0.941. This could be an indication of smaller differences between the observed data and the fitted values generated from the linear regression model. 

Furthermore, it is 95% possible that the 2.5% quantile of 2.058 and 97.5% quantile of 2.139 of Log(β0*β1) could indicate an positive correlation between tmin and tmax.

## Problem 3

### Data Cleaning

After importing data of birth weight, variables `babysex`, `malform`, `frace`, and `mrace` are transformed into factor variables. 

```{r data import 3}
birthweight_df = 
  read_csv(
    "data/birthweight.csv", na = c("", "NA", "Unknown")
    ) |> 
  mutate(
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  ) 
```

### Proposed Linear Regression Model

```{r all variable}
lm(bwt ~ babysex + bhead + blength + bwt + delwt + fincome + frace + gaweeks+malform+menarche+mheight+momage+mrace+parity+pnumlbw+pnumsga+ppbmi+ppwt+smoken+wtgain, data = birthweight_df) |> broom::tidy()
```

Before proposing a linear regression model, a linear model using `bwt` as the outcome and all other variables as predictors is generated below. Through comparing p values of all predictors, we can observe that variables `bhead`, `blength`, and `delwt` has smaller p values compared to other variables. Based on these smaller p values, these three variables are proposed for the first linear model.

```{r my model}
birthweight_df_select = birthweight_df |> 
  select(bwt, blength, bhead, gaweeks, babysex, delwt)
  
linear_mod_1 = lm(bwt ~ bhead + blength + delwt+ bhead*blength*delwt, data = birthweight_df_select)
```

After generating the first linear regression model, predictions and residues for this model are generated `using add_predictions` and `add_residues`. Next, a plot of model residuals against fitted values is generated below. We can observe a slow decrease in residues as fitted value increases. 

```{r my model residue pred plot}
birthweight_df_select |> 
  modelr::add_predictions(linear_mod_1) |> 
  modelr::add_residuals(linear_mod_1) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .5)+
  geom_smooth(color = "red", se=FALSE)
```

### Model 2 and Model 3

```{r linear mod 2}
linear_mod_2 = 
  lm(bwt ~ blength + gaweeks, data = birthweight_df_select) 
```

Using `lm()`, a second linear model is generated uisng `bwt` as the outcome and `blength` and `gaweeks` as the predictors.

```{r linear mod 3}
linear_mod_3 = 
  lm(bwt ~ bhead* blength + blength* babysex + blength*bhead*babysex, data = birthweight_df_select)
```

For the third linear model is generated uisng `bwt` as the outcome and `blength`, `bhead`, `gaweeks`, and their interactions as the predictors.

### Comparing the 3 Models:

After generating the three models, cross validation is used through `croosv_mc` to compare the three models. First, `train` and `test` variables are setted up.

```{r cv setup}
cv_df =
  crossv_mc(birthweight_df_select, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

Next, using `map()`, prediction errors based on train and test samples are obtained for each model and stored in the `cv_df` data frame.

```{r cv for three models}
cv_df = 
  cv_df |> 
  mutate(
    mod_1 = map(train, \(df) lm(bwt ~ bhead + blength + delwt+ bhead*blength*delwt, data = df)),
    mod_2 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    mod_3  = map(train, \(df) lm(bwt ~ bhead* blength + blength* babysex + blength*bhead*babysex, data = df))
    ) |> 
  mutate(
    rmse_1 = map2_dbl(mod_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2    = map2_dbl(mod_2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_3 = map2_dbl(mod_3, test, \(mod, df) rmse(model = mod, data = df)))

```

The average value of prediction error of model 1 is `r cv_df |> pull(rmse_1) |> mean()`. The average value of prediction error of model 2 is `r cv_df |> pull(rmse_2) |> mean()`. The average value of prediction error of model 3 is `r cv_df |> pull(rmse_3) |> mean()`. Overall, model 1 has the lowest prediction error and model 2 has the highest prediction error. This pattern could also be observed from the plot below. This could be led by a larger number of variables with smaller p values included in the first model compared to the second model.

```{r rmse plot}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

