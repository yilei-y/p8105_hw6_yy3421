---
title: "p8105_hw6_yy3421"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

## Problem 1

```{r data import 1 and cleaning}
homicide_df = 
  read_csv("data/homicide-data.csv", na = c("", "NA", "Unknown"))  |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c( "Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

In the data cleaning code below we create a `city_state` variable. Using the mutate function, `victim_age` is transformed into numerical values and `resolution` is created based on the `disposition` variable. Then, `victim_race` and `city_state` variables are each filtered based on this problem requirement. At the end, five variables above are selected for further analysis.

```{r Baltimore}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

In the above code chunk, a logistic regression model is used for data from Baltimore, MD using `resolved` as the outcome and `victim_age`, `victim_sex`, and `victim_race` as predictors. After saving the output, estimate and confidence interval of the adjusted odds ratio is obtained after using `broom::tidy` and `mutate` for comparison between non-white victims and white victims.

```{r all cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

Next, through using`nest()`, `map()`, and `unnest()`, the model for Baltimore is fitted for each of the cities. The adjusted odds ratio (and CI) for solving homicides comparing non-white victims to white victims for each cities are then obtained and stored. 

```{r problem 1 plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
Above, a plot is generated for the estimated ORs and CIs for each city, ordered by magnitude of the OR from smallest to largest. From this plot, as most cities have odds ratios that are smaller than 1, it could be intepreted that crimes with male victims have smaller odds of resolution compared to crimes with female victims after adjusting for victim age and race. However, there is a maximum disparity in New York. Further, a narrow confidence interval in approximately half of the cities could suggest a significant difference in resolution rates by sex after adjustment for victim age and race.


## Problem 2

```{r data import 2}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

After importing the weather data, a `boot_sample` function is first created for generate bootstrap samples.

```{r boot_sample}

boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
  
}

```

Below, using the `boot_sample` and `map` functions, 5000 boot samples are created and stored in the `boot_straps` dataframe. Using the `map` and `lm` function, linear models using `tmax` as the outcome and `tmin`and `prcp` as predictors are stored in the `models` variable. 

Next, using `map` and `broom::glance`, we can generate results of the linear model for each boot sample. After using the `unnest` function to show this result, we can specifically select the `r.squared` variable to obtain the estimates. Similarly, using `map` and `broom::tidy`, we can obtain estimates for β0, β1, and β2. 


```{r boot strap}
boot_straps = 
  tibble(strap_number = 1:5000) |> 
  mutate(
    strap_sample = map(strap_number, \(i) boot_sample(weather_df)),
    models = map(strap_sample, \(df) lm(tmax ~ tmin + prcp, data = df))
  )

boot_results_1 = 
  boot_straps |> 
  mutate(
    results_1 = map(models, broom::glance)
  ) |> 
  select(strap_number, results_1) |> 
  unnest(results_1) |> 
  select(strap_number, r.squared)

boot_results_2 = 
  boot_straps |> 
  mutate(
    results_2 = map(models, broom::tidy),
    results_2 = map(results_2, as_tibble)
  ) |> 
  select(strap_number, results_2) |> 
  unnest(results_2) |>
  select(-std.error,-statistic, -p.value) |> 
  pivot_wider(
    names_from = term,
    values_from = estimate
  )

```

The two results of r.squared and β values are combined into one data frame using the left_join function. Using the mutate function, log(β0β1) andlog(β1β2) are calcualted for each boot sample.

```{r result combination and calc}
boot_results = 
  left_join(boot_results_1, boot_results_2, by = "strap_number") |> 
  janitor::clean_names() |> 
  rename(beta_zero = intercept,
         beta_one = tmin,
         beta_two = prcp) |> 
  mutate(
    logbeta01 = log(beta_zero * beta_one),
    logbeta12 = log(abs(beta_one * beta_two))
  )
```

### Distribution of R.Squared and Log(β1*β2)

The distribution for R.Squared and Log(β1*β2) are plotted below. We can observe a tail for smaller estimates for both two quantities, with a heavier tail for estimates of R.squared, which could be an indication for outliers in the bootstrap samples.

```{r plots problem 2}
boot_results |> 
  ggplot(aes(x = r_squared))+
  geom_density()+
  labs(
    x = "R Squared",
    caption = "Distribution of Estimates of R.Squared"
  )

boot_results |> 
  ggplot(aes(x = logbeta12))+
  geom_density()+
  labs(
    x = "Log(β1*β2)",
    caption = "Distribution of Estimates of Log(β1*β2)"
  )
```

### 2.5% and 97.5% Quantiles Summary for R Squared and and Log(β0*β1)

Using `summarize()`, we can generate 2.5% and 97.5% quantiles for R.squared and Log(β0*β1). 

```{r quantiles summary}
boot_results |> 
  summarize(
    ci_lower_r = quantile(r_squared, 0.025),
    ci_upper_r = quantile(r_squared, 0.975),
    ci_lower_beta01 = quantile(logbeta01, 0.025),
    ci_upper_beta01 = quantile(logbeta01, 0.975)
  )
```

Based on the quantile summary, we can observe an overall high R squared with its 2.5% quantile of 0.888 and 97.5% quantile of 0.940. This could be an indication of smaller differences between the observed data and the fitted values generated from the linear regression model. 

Furthermore, it is 95% oissubke tgat the 2.5% quantile of 2.06 and 97.5% quantile of 2.14 of Log(β0*β1) could indicate an positive correlation between tmin and tmax.

## Problem 3

### Data Cleaning

After importing data of birth weight, variables `babysex`, `malform`, `frace`, and `mrace` are transformed into factor variables. 

```{r data import 3}
birthweight_df = 
  read_csv(
    "data/birthweight.csv", na = c("", "NA", "Unknown")
    ) |> 
  mutate(
    babysex = as.factor(babysex),
    malform = as.factor(malform),
    frace = as.factor(frace),
    mrace = as.factor(mrace)
  ) 

            
```

    babysex = case_match(
      babysex, 
      1 ~ "male",
      2 ~ "female"
    ),
    babysex = as.factor(babysex),
    malform = case_match(
      malform, 
      0 ~ "absent",
      1 ~ "present"
    ),

### Proposed Linear Regression Model

# Before proposing a linear regression model, a linear model using bwt as the outcome and all other variables as predictors is generated below. Through comparing ... of all predictors, ....

```{r}
linear_mod_all = lm(bwt ~ babysex + bhead + blength + bwt + delwt + fincome + frace + gaweeks+malform+menarche+mheight+momage+mrace+parity+pnumlbw+pnumsga+ppbmi+ppwt+smoken+wtgain, data = birthweight_df) |> broom::tidy()
```

Based on the data above, baby length, baby sex, and presence of malformations are selected as predictors for the first model. 

```{r my model}
birthweight_df_select = birthweight_df |> 
  select(bwt, blength, bhead, gaweeks, babysex, delwt)
  

linear_mod_1 = lm(bwt ~ bhead + blength + delwt+ bhead*blength*delwt, data = birthweight_df_select)
```

# Describe your modeling process and show a plot of model residuals against fitted values – use add_predictions and add_residuals in making this plot.
After generating the first linear regression model, predictions and residues for this model are generated `using add_predictions` and `add_residues`. The distribution of residues 

```{r my model residue pred plot}
birthweight_df_select |> 
  modelr::add_predictions(linear_mod_1) |> 
  modelr::add_residuals(linear_mod_1) |> 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(alpha = .5)+
  geom_smooth(color = "red", se=FALSE)
```



### One using length at birth and gestational age as predictors (main effects only)
```{r linear mod 2}
linear_mod_2 = 
  lm(bwt ~ blength + gaweeks, data = birthweight_df_select) 
```

###One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r linear mod 3}
linear_mod_3 = 
  lm(bwt ~ bhead* blength + bhead* babysex+ bhead*blength*babysex, data = birthweight_df_select) 
```

```{r compare 3 models}
birthweight_df_select |> 
  gather_predictions(linear_mod_1, linear_mod_2, linear_mod_3) |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = blength, y = bwt)) + 
  geom_point(alpha = .5) +
  geom_line(aes(y = pred), color = "red") + 
  facet_grid(~model)
```


```{r cv setup}
cv_df =
  crossv_mc(birthweight_df_select, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```


```{r cv for three models}
cv_df = 
  cv_df |> 
  mutate(
    mod_1 = map(train, \(df) lm(bwt ~ bhead + blength + delwt+ bhead*blength*delwt, data = df)),
    mod_2 = map(train, \(df) lm(bwt ~ blength + gaweeks, data = df)),
    mod_3  = map(train, \(df) lm(bwt ~ bhead* blength + bhead* babysex+ bhead*blength*babysex, data = df))
    ) |> 
  mutate(
    rmse_1 = map2_dbl(mod_1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_2    = map2_dbl(mod_2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_3 = map2_dbl(mod_3, test, \(mod, df) rmse(model = mod, data = df)))
```

```{r rmse plot}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

